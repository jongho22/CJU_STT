{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"|1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import base64\n",
    "from io import *\n",
    "import struct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 메인 모델\n",
    "model = whisper.load_model(\"medium\")\n",
    "model = model.to(device)\n",
    "# 작은 모델\n",
    "# small_model = whisper.load_model(\"tiny\")\n",
    "# small_model = small_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(\"./audio_with_header.wav\", fp16=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/jongho/바탕화면/테스트 음성파일/\"\n",
    "file_name = \"연제리2.wav\"\n",
    "\n",
    "encode_audio = base64.b64encode(open(f\"{file_path}{file_name}\", \"rb\").read()).decode('utf-8') \n",
    "\n",
    "# result = small_model.transcribe(f\"{file_path}{file_name}\" , fp16=False)\n",
    "# print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wav_header(binary_data):\n",
    "    # WAV 파일 헤더 구조에 맞게 필드를 읽음\n",
    "  \n",
    "    chunk_id = binary_data[:4]\n",
    "    chunk_size = struct.unpack('<I', binary_data[4:8])[0]\n",
    "    format_str = binary_data[8:12]\n",
    "    subchunk1_id = binary_data[12:16]\n",
    "    subchunk1_size = struct.unpack('<I', binary_data[16:20])[0]\n",
    "    audio_format = struct.unpack('<H', binary_data[20:22])[0]\n",
    "    num_channels = struct.unpack('<H', binary_data[22:24])[0]\n",
    "    sample_rate = struct.unpack('<I', binary_data[24:28])[0]\n",
    "    byte_rate = struct.unpack('<I', binary_data[28:32])[0]\n",
    "    block_align = struct.unpack('<H', binary_data[32:34])[0]\n",
    "    bits_per_sample = struct.unpack('<H', binary_data[34:36])[0]\n",
    "    subchunk2_id = binary_data[36:40]\n",
    "    subchunk2_size = struct.unpack('<I', binary_data[40:44])[0]\n",
    "    \n",
    "\n",
    "    header = {\"Chunk_ID\": chunk_id,\n",
    "      \"Chunk_Size\" : chunk_size,\n",
    "      \"Format\" : format_str,\n",
    "      \"Subchunk1_ID\" : subchunk1_id,\n",
    "      \"Subchunk1_Size\" : subchunk1_size,\n",
    "      \"Audio_Format\" : audio_format,\n",
    "      \"Num_Channels\" : num_channels,\n",
    "      \"Sample_Rate\" : sample_rate,\n",
    "      \"Byte_Rate\" : byte_rate,\n",
    "      \"Block_Align\" : block_align,\n",
    "      \"Bits_Per_Sample\" : bits_per_sample,\n",
    "      \"Subchunk2_ID\" : subchunk2_id,\n",
    "      \"Subchunk2_Size\" : subchunk2_size}\n",
    "    return header \n",
    "\n",
    "def make_header(wav_header, input_data) :\n",
    "    chunk_id = wav_header['Chunk_ID']\n",
    "    chunk_size = len(input_data) + 44  # 데이터 크기에 추가 정보 크기(46바이트)를 더함\n",
    "    format_str = wav_header['Format']\n",
    "    subchunk1_id = wav_header['Subchunk1_ID']\n",
    "    subchunk1_size = wav_header['Subchunk1_Size']  # 일반적인 값을 사용함\n",
    "    audio_format = wav_header['Audio_Format'] # PCM 포맷\n",
    "    num_channels = wav_header['Num_Channels']  # 스테레오\n",
    "    sample_rate = wav_header['Sample_Rate']  # 예시로 44100Hz\n",
    "    byte_rate = wav_header['Byte_Rate']\n",
    "    block_align = wav_header['Block_Align']    # 채널 수 * 샘플 크기(2바이트)\n",
    "    bits_per_sample = wav_header['Bits_Per_Sample']   # 16비트 PCM 사용\n",
    "    subchunk2_id =  b'data'\n",
    "    subchunk2_size = len(input_data) + 44  # 데이터 크기\n",
    "\n",
    "    wav_header = struct.pack('<4sI4s4sIHHIIHH4sI', chunk_id, chunk_size, format_str, subchunk1_id,\n",
    "                            subchunk1_size, audio_format, num_channels, sample_rate, byte_rate,\n",
    "                            block_align, bits_per_sample, subchunk2_id, subchunk2_size)\n",
    "    \n",
    "    return wav_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 소켓을 통해서 base64 데이터를 수신 받았을때\n",
    "# 바이너리 데이터로 변환\n",
    "decoded_audio = base64.b64decode(encode_audio)\n",
    "\n",
    "# 오디오 데이터에서 헤더정보 추출\n",
    "BINARY_DATA = decoded_audio[:44]\n",
    "audio_signal_data = decoded_audio[44:]\n",
    "\n",
    "wav_header = extract_wav_header(BINARY_DATA)\n",
    "\n",
    "print(\"음성파일 헤더 정보 :\",wav_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할 사이즈\n",
    "SPLIT_SIZE = 10000000 # 10MB\n",
    "# 기본 경로\n",
    "BASE_PATH = \"/home/jongho/Spring_project/CJU_STT/CJU_Web_Application_Services/src/main/webapp/resources/python/result\"\n",
    "# 분할 바이너리 데이터 리스트\n",
    "split_data = []\n",
    "\n",
    "for i in range(0, len(audio_signal_data), SPLIT_SIZE):\n",
    "    split_data.append(audio_signal_data[i:i+SPLIT_SIZE])\n",
    "\n",
    "print(f\"바이너리 데이터 분할 완료 : {len(split_data)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 20\n",
    "import numpy as np\n",
    "\n",
    "# WAV 파일 헤더 정보 생성\n",
    "new_header = make_header(wav_header, split_data[i])\n",
    "audio_with_header = new_header + split_data[i]\n",
    "\n",
    "# 오디오 파일 저장 => 테스트 용\n",
    "with open(\"audio_with_header.wav\", \"wb\") as f:\n",
    "    f.write(audio_with_header)\n",
    "\n",
    "audio_array = np.frombuffer(audio_with_header, dtype=np.float32)\n",
    "audio_array = np.copy(audio_array)\n",
    "audio_tensor = torch.from_numpy(audio_array)\n",
    "# with open(\"check.txt\", \"w+\") as f:\n",
    "#     f.write(str(audio_array))\n",
    "\n",
    "aud_array = np.frombuffer(audio_with_header, np.int8).flatten().astype(np.float32) / 32768.0\n",
    "\n",
    "# STT 실행\n",
    "result = model.transcribe(aud_array, fp16=False)\n",
    "print(result['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "\n",
    "# 오디오 파일 생성을 위한 파라미터 설정\n",
    "sample_width = 2  # 16비트 오디오를 사용할 경우 2바이트\n",
    "frame_rate = 44100  # 샘플링 주파수\n",
    "num_channels = 2  # 스테레오 오디오인 경우\n",
    "num_frames = len(audio_with_header) // (sample_width * num_channels)  # 오디오 길이 계산\n",
    "\n",
    "# 오디오 파일 생성\n",
    "with wave.open('output_audio.wav', 'wb') as wf:\n",
    "    wf.setnchannels(num_channels)\n",
    "    wf.setsampwidth(sample_width)\n",
    "    wf.setframerate(frame_rate)\n",
    "    wf.setnframes(num_frames)\n",
    "    wf.writeframes(audio_with_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(audio_tensor, fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(split_data)) :\n",
    "\n",
    "#     # WAV 파일 헤더 정보 생성\n",
    "#     new_header = make_header(wav_header, split_data[i])\n",
    "#     audio_with_header = new_header + split_data[i]\n",
    "\n",
    "#     # Whiper 입력 데이터 형식으로 변환\n",
    "#     audio = AudioSegment.from_file(BytesIO(audio_with_header), format=\"wav\")\n",
    "#     data = numpy.array(audio.get_array_of_samples())\n",
    "#     audio_float32 = data.astype('float32') / 32768.0\n",
    "\n",
    "#     # STT 실행\n",
    "#     result = model.transcribe(audio_float32, fp16=False)\n",
    "#     model.l\n",
    "\n",
    "#     print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for audio_file_name in work_list :\n",
    "#     save_path = \"/home/jongho/Spring_project/CJU_STT/CJU_Web_Application_Services/src/main/webapp/resources/python/result\"\n",
    "#     target_audio_path = f\"{save_path}/{audio_file_name}\"\n",
    "#     print(target_audio_path)\n",
    "#     result = model.transcribe(target_audio_path, fp16=False)\n",
    "#     print(result['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
